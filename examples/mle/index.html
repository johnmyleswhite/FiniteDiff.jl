<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="johnmyleswhite">
  
  <title>Maximum Likelihood Estimation - FiniteDiff.jl</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../assets/Documenter.css" rel="stylesheet">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Maximum Likelihood Estimation";
    var mkdocs_page_input_path = "examples/mle.md";
    var mkdocs_page_url = "/examples/mle/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
  <script src="../../assets/mathjaxhelper.js"></script>

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> FiniteDiff.jl</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Core Functions</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../core/derivative/">derivative</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../core/second_derivative/">second_derivative</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../core/gradient/">gradient</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../core/hessian/">hessian</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Developer Documentation</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../developer/internal_functions/">Internal Types and Functions</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../developer/code_walkthrough/">Code Walkthrough</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Examples</span></li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Maximum Likelihood Estimation</a>
        
            <ul>
            
            </ul>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../bibliography/">Bibliography</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../LICENSE/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">FiniteDiff.jl</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Examples &raquo;</li>
        
      
    
    <li>Maximum Likelihood Estimation</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/johnmyleswhite/FiniteDiff.jl/" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>In this example, we show how to use FiniteDiff.jl to compute the maximum likelihood estimates for a univariate logistic regression model. We'll use FiniteDiff to approximate the gradients as part of our model fitting process. We'll also use FiniteDiff to compute an approximate Hessian, which we will use as an approximation of the observed Fisher information matrix that defines the standard errors for the logistic regression model. At the end, we'll compare our results with results computed in R as a way of showing that our work is correct – and that the approximations introduced by FiniteDiff are not the dominant source of errors in these computations.</p>
<p>The first thing we'll do is import all of the libraries we'll use. If you do not have these libraries installed, you'll need to use <code>Pkg.add</code> to install them before you can continue on with this example.</p>
<div class="codehilite"><pre><span></span><span class="k">import</span> <span class="n">Distributions</span><span class="p">:</span> <span class="n">Normal</span><span class="p">,</span> <span class="n">Bernoulli</span>
<span class="k">import</span> <span class="n">FiniteDiff</span><span class="p">:</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">hessian</span>
<span class="k">import</span> <span class="n">Optim</span><span class="p">:</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">,</span> <span class="n">minimizer</span>
<span class="k">import</span> <span class="n">RCall</span><span class="p">:</span> <span class="p">@</span><span class="n">R_str</span>
</pre></div>


<p>The link function for a logistic regression model is the inverse logit, so we define a function to compute this quantity:</p>
<div class="codehilite"><pre><span></span><span class="n">invlogit</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>


<p>Next, we'll define a function to simulate data from a logistic regression model:</p>
<div class="codehilite"><pre><span></span><span class="k">function</span><span class="nf"> simulate_logistic_regression</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">β₀</span><span class="p">,</span> <span class="n">β₁</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n_obs</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">Array</span><span class="p">(</span><span class="kt">Float64</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">n_obs</span>
        <span class="n">z_i</span> <span class="o">=</span> <span class="n">β₀</span> <span class="o">+</span> <span class="n">β₁</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">p_i</span> <span class="o">=</span> <span class="n">invlogit</span><span class="p">(</span><span class="n">z_i</span><span class="p">)</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">p_i</span><span class="p">))</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="k">end</span>
</pre></div>


<p>Once we have data available, we can define the negative log likelihood function using a closure that is a function of the parameter vector <code>Θ</code>, but which wraps the observed values of <code>x</code> and <code>y</code> so that <code>Θ</code> is the function's only argument:</p>
<div class="codehilite"><pre><span></span><span class="k">function</span><span class="nf"> make_nll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">function</span><span class="nf"> nll</span><span class="p">(</span><span class="n">θ</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="p">:</span><span class="n">length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">invlogit</span><span class="p">(</span><span class="n">θ</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">θ</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">-=</span> <span class="n">ifelse</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span>
        <span class="k">end</span>
        <span class="n">s</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">nll</span>
<span class="k">end</span>
</pre></div>


<p>We'll also want to be able to compute gradients. This is the first place in which we'll use FiniteDiff. Because Optim requires that the gradient function employ a different calling convention than the gradient functions that are generated by FiniteDiff, we'll need to wrap the results in a calling-convention translation layer:</p>
<div class="codehilite"><pre><span></span><span class="k">function</span><span class="nf"> make_nll_gr</span><span class="o">!</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="n">initial_x</span><span class="p">)</span>
    <span class="n">tmp!</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="n">mutates</span><span class="o">=</span><span class="n">true</span><span class="p">)</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">similar</span><span class="p">(</span><span class="n">initial_x</span><span class="p">)</span>

    <span class="n">nll_gr!</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">begin</span>
        <span class="n">tmp!</span><span class="p">(</span><span class="n">gr</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">buffer</span><span class="p">);</span>
        <span class="k">return</span>
    <span class="k">end</span>

    <span class="k">return</span> <span class="n">nll_gr!</span>
<span class="k">end</span>
</pre></div>


<p>Now that we have the core machinery for model fitting in place, we can simulate some data and fit our model to it:</p>
<div class="codehilite"><pre><span></span><span class="c"># Simulate data</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">simulate_logistic_regression</span><span class="p">(</span><span class="mi">2500</span><span class="p">,</span> <span class="mf">0.017</span><span class="p">,</span> <span class="mf">0.217</span><span class="p">)</span>

<span class="c"># Construct the negative log likelihood function.</span>
<span class="n">nll</span> <span class="o">=</span> <span class="n">make_nll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c"># Initialize our model parameters as a vector of all zeros.</span>
<span class="n">initial_x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>

<span class="c"># Construct the gradient of the negative log likelihood (NLL) function.</span>
<span class="n">nll_gr!</span> <span class="o">=</span> <span class="n">make_nll_gr!</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="n">initial_x</span><span class="p">)</span>

<span class="c"># Pass the NLL function and its gradient to Optim.optimize to find the MLE.</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="n">nll_gr!</span><span class="p">,</span> <span class="n">initial_x</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">())</span>

<span class="c"># Extract the coefficients as the minimizer of the NLL function.</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">minimizer</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c"># Evaluate the Hessian of the NLL function at the coefficients to approximate</span>
<span class="c">#     the Fisher information matrix.</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="n">coefficients</span><span class="p">)</span>

<span class="c"># Use an analytic result relating the Hessian to the standard errors to compute</span>
<span class="c">#     the standard errors for each coefficient.</span>
<span class="n">standard_errors</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">diag</span><span class="p">(</span><span class="n">inv</span><span class="p">(</span><span class="n">H</span><span class="p">)))</span>

<span class="c"># Write our data to disk so we can confirm our results in R.</span>
<span class="n">writecsv</span><span class="p">(</span><span class="s">&quot;example_data.csv&quot;</span><span class="p">,</span> <span class="n">hcat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c"># Use the RCall package&#39;s R macro to run R&#39;s glm function, which matches our</span>
<span class="c"># results.</span>
<span class="n">R</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">summary(</span>
<span class="s">    glm(</span>
<span class="s">        V2 ~ V1,</span>
<span class="s">        data = read.csv(&#39;example_data.csv&#39;, header = FALSE),</span>
<span class="s">        family = binomial(link = &quot;</span><span class="n">logit</span><span class="s">&quot;)</span>
<span class="s">    )</span>
<span class="s">)</span>
<span class="s">&quot;&quot;&quot;</span>

<span class="c"># Before terminating the program, clean up the file we created.</span>
<span class="n">rm</span><span class="p">(</span><span class="s">&quot;example_data.csv&quot;</span><span class="p">)</span>
</pre></div>


<p>For readers interested in model fitting, we hope this example illustrates the power of FiniteDiff for approximating gradients and Hessians. In this example, it is easy to look up analytic forms for the gradient and Hessian, but in more complicated models the use of numeric differentiation can save time while testing out new models.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../bibliography/" class="btn btn-neutral float-right" title="Bibliography">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../developer/code_walkthrough/" class="btn btn-neutral" title="Code Walkthrough"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/johnmyleswhite/FiniteDiff.jl/" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../developer/code_walkthrough/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../bibliography/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
