{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThe FiniteDiff.jl package provides functions for computing approximate first and second derivatives of univariate and multivariate functions. It uses numerical differentiation to generate these approximations, which means that it is possible to compute an approximate derivative for any Julia function \u2013 even functions that are not differentiable and are therefore not safe to differentiate.\n\n\nDespite being universally applicable, the quality of the approximated derivatives computed by this package varies substantially across different functions. Even for a fixed function, the quality of the approximations provided by numerical differentiation varies substantially across different points in the function's domain. If your function is amenable to other techniques (such as forward-mode automatic differentiation), you will almost certainly get better results from those techniques. Use this package only when you need approximate derivatives and no other technique will work.\n\n\nAn example of a setting in which you would need to use numerical differentiation involves a Julia function that calls out to a C function to compute a mathematical function along the way. This might happen if you were to use a C implementation to compute the integral of the incomplete beta function as part of a statistical calculation. Because the C function is not written in pure Julia, automatic differentiation techniques would not work on the Julia code as a whole. In that kind of setting, numerical differentiation would be the only effective way to automatically determine the Julia function's derivatives.\n\n\nThis package is intended to be a replacement for the numerical differentiation functionality found in the Calculus package. It attempts to unify the API for numerical differentiation with the API for automatic differentiation found in the \nForwardDiff.jl package\n.\n\n\n\n\nBasic Examples\n\n\nThe examples below are intended to provide a basic sense of how the package can be used. Further documentation is found in others sections.\n\n\njulia\n \nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nsecond_derivative\n,\n \ngradient\n,\n \nhessian\n\n\n\njulia\n \ny\u2032\n \n=\n \nderivative\n(\nsin\n,\n \n1.0\n)\n\n\n0.5403023058631036\n\n\n\njulia\n \ny\u2032\u2032\n \n=\n \nsecond_derivative\n(\nsin\n,\n \n1.0\n)\n\n\n-\n0.8414709866046906\n\n\n\njulia\n \ngr\n \n=\n \ngradient\n(\nx\n \n-\n \n(\n1.0\n \n-\n \nx\n[\n1\n])\n^\n2\n \n+\n \n(\n2.0\n \n-\n \nx\n[\n2\n])\n^\n2\n,\n \n[\n0.0\n,\n \n0.0\n])\n\n\n2\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n-\n2.0\n\n \n-\n4.0\n\n\n\njulia\n \nH\n \n=\n \nhessian\n(\nx\n \n-\n \n(\n1.0\n \n-\n \nx\n[\n1\n])\n^\n2\n \n+\n \n(\n2.0\n \n-\n \nx\n[\n2\n])\n^\n2\n,\n \n[\n0.0\n,\n \n0.0\n])\n\n\n2\n\u00d72\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n2.0\n         \n6.05545e-6\n\n \n6.05545e-6\n  \n2.0\n\n\n\n\n\n\nThe four functions shown above define the core API for the FiniteDiff package. Each of the functions has a section dedicated to it; that function's section provides a full overview of the function's use.\n\n\nBefore you read the detailed documentation, we encourage you to consider using mutating variants of the \ngradient\n and \nhessian\n functions to avoid allocating memory unnecessarily when working with multivariate functions. The examples below show how these mutating variants work:\n\n\njulia\n \nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nhessian!\n\n\n\n# Allocate memory once that can be reused in the future.\n\n\njulia\n \ngr\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\n2\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n1.4822e-323\n\n \n4.44659e-323\n\n\n\njulia\n \nH\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n,\n \n2\n)\n\n\n2\n\u00d72\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n1.50626e165\n  \n6.97302e252\n\n \n6.46167e174\n  \n8.40996e-315\n\n\n\njulia\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\n2\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n2.27838e-314\n\n \n2.25293e-314\n\n\n\njulia\n \nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\n2\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n0.0\n\n \n0.0\n\n\n\njulia\n \ngradient!\n(\ngr\n,\n \nx\n \n-\n \n(\n1.0\n \n-\n \nx\n[\n1\n])\n^\n2\n \n+\n \n(\n2.0\n \n-\n \nx\n[\n2\n])\n^\n2\n,\n \nx\n,\n \nbuffer\n)\n\n\n\n# The gradient is now stored in gr.\n\n\njulia\n \ngr\n\n\n2\n-\nelement\n \nArray\n{\nFloat64\n,\n1\n}:\n\n \n-\n2.0\n\n \n-\n4.0\n\n\n\njulia\n \nhessian!\n(\nH\n,\n \nx\n \n-\n \n(\n1.0\n \n-\n \nx\n[\n1\n])\n^\n2\n \n+\n \n(\n2.0\n \n-\n \nx\n[\n2\n])\n^\n2\n,\n \nx\n,\n \nbuffer\n)\n\n\n\n# The Hessian is now stored in H.\n\n\njulia\n \nH\n\n\n2\n\u00d72\n \nArray\n{\nFloat64\n,\n2\n}:\n\n \n2.0\n         \n6.05545e-6\n\n \n6.05545e-6\n  \n2.0\n\n\n\n\n\n\nNote that these functions require an additional buffer of memory to operate on. This buffer will be mutated, but its output at the end of the function's execution is essentially meaningless. It is just a scratchpad used to speed up the computation of the numerical approximations.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "The FiniteDiff.jl package provides functions for computing approximate first and second derivatives of univariate and multivariate functions. It uses numerical differentiation to generate these approximations, which means that it is possible to compute an approximate derivative for any Julia function \u2013 even functions that are not differentiable and are therefore not safe to differentiate.  Despite being universally applicable, the quality of the approximated derivatives computed by this package varies substantially across different functions. Even for a fixed function, the quality of the approximations provided by numerical differentiation varies substantially across different points in the function's domain. If your function is amenable to other techniques (such as forward-mode automatic differentiation), you will almost certainly get better results from those techniques. Use this package only when you need approximate derivatives and no other technique will work.  An example of a setting in which you would need to use numerical differentiation involves a Julia function that calls out to a C function to compute a mathematical function along the way. This might happen if you were to use a C implementation to compute the integral of the incomplete beta function as part of a statistical calculation. Because the C function is not written in pure Julia, automatic differentiation techniques would not work on the Julia code as a whole. In that kind of setting, numerical differentiation would be the only effective way to automatically determine the Julia function's derivatives.  This package is intended to be a replacement for the numerical differentiation functionality found in the Calculus package. It attempts to unify the API for numerical differentiation with the API for automatic differentiation found in the  ForwardDiff.jl package .", 
            "title": "Introduction"
        }, 
        {
            "location": "/#basic-examples", 
            "text": "The examples below are intended to provide a basic sense of how the package can be used. Further documentation is found in others sections.  julia   import   FiniteDiff :   derivative ,   second_derivative ,   gradient ,   hessian  julia   y\u2032   =   derivative ( sin ,   1.0 )  0.5403023058631036  julia   y\u2032\u2032   =   second_derivative ( sin ,   1.0 )  - 0.8414709866046906  julia   gr   =   gradient ( x   -   ( 1.0   -   x [ 1 ]) ^ 2   +   ( 2.0   -   x [ 2 ]) ^ 2 ,   [ 0.0 ,   0.0 ])  2 - element   Array { Float64 , 1 }: \n  - 2.0 \n  - 4.0  julia   H   =   hessian ( x   -   ( 1.0   -   x [ 1 ]) ^ 2   +   ( 2.0   -   x [ 2 ]) ^ 2 ,   [ 0.0 ,   0.0 ])  2 \u00d72   Array { Float64 , 2 }: \n  2.0           6.05545e-6 \n  6.05545e-6    2.0   The four functions shown above define the core API for the FiniteDiff package. Each of the functions has a section dedicated to it; that function's section provides a full overview of the function's use.  Before you read the detailed documentation, we encourage you to consider using mutating variants of the  gradient  and  hessian  functions to avoid allocating memory unnecessarily when working with multivariate functions. The examples below show how these mutating variants work:  julia   import   FiniteDiff :   gradient! ,   hessian!  # Allocate memory once that can be reused in the future.  julia   gr   =   Array ( Float64 ,   2 )  2 - element   Array { Float64 , 1 }: \n  1.4822e-323 \n  4.44659e-323  julia   H   =   Array ( Float64 ,   2 ,   2 )  2 \u00d72   Array { Float64 , 2 }: \n  1.50626e165    6.97302e252 \n  6.46167e174    8.40996e-315  julia   buffer   =   Array ( Float64 ,   2 )  2 - element   Array { Float64 , 1 }: \n  2.27838e-314 \n  2.25293e-314  julia   x   =   [ 0.0 ,   0.0 ]  2 - element   Array { Float64 , 1 }: \n  0.0 \n  0.0  julia   gradient! ( gr ,   x   -   ( 1.0   -   x [ 1 ]) ^ 2   +   ( 2.0   -   x [ 2 ]) ^ 2 ,   x ,   buffer )  # The gradient is now stored in gr.  julia   gr  2 - element   Array { Float64 , 1 }: \n  - 2.0 \n  - 4.0  julia   hessian! ( H ,   x   -   ( 1.0   -   x [ 1 ]) ^ 2   +   ( 2.0   -   x [ 2 ]) ^ 2 ,   x ,   buffer )  # The Hessian is now stored in H.  julia   H  2 \u00d72   Array { Float64 , 2 }: \n  2.0           6.05545e-6 \n  6.05545e-6    2.0   Note that these functions require an additional buffer of memory to operate on. This buffer will be mutated, but its output at the end of the function's execution is essentially meaningless. It is just a scratchpad used to speed up the computation of the numerical approximations.", 
            "title": "Basic Examples"
        }, 
        {
            "location": "/core/derivative/", 
            "text": "Description\n\n\nThe \nderivative\n function is used to compute the derivative of a univariate function that maps $\\mathbb{R}$ to $\\mathbb{R}$. For example, \nx -\n sin(x)\n is a univariate function that might be passed to the \nderivative\n function.\n\n\nThe \nderivative\n function comes in three core variants:\n\n\n\n\nA pure function that directly computes the derivative of a function \nf\n at a   fixed numeric value of \nx\n and returns a number as a result.\n\n\nA mutating function that computes the derivative of a function \nf\n at a fixed   numeric value of \nx\n and stores the result into a user-provided output   array. This function returns \nnothing\n since its action is focused on   mutation.\n\n\nA higher-order function that constructs a new function \nf_prime\n that will   compute the derivative of \nf\n at any point \nx\n that is provided as an   argument to \nf_prime\n. This newly constructed function is the return value   for this variant of the \nderivative\n function.\n\n\n\n\nIn addition, the \nderivative\n function allows one to chose the mode of numerical differentiation that is used to compute an approximate derivative. The default is use central finite differences, which evaluates \nf\n at two nearby points and provides much higher accuracy than other strategies at a cost of roughly double the amount of computation. Other modes of numerical differentiation are available, but must be opted-in to explicitly.\n\n\n\n\nPrimary Methods\n\n\nThe primary methods that most users will want to use are the following:\n\n\n\n\nUse \nderivative(f::Function, x::AbstractFloat)\n to approximate the derivative   of \nf\n at \nx\n. This is the pure variant described above. Calling this   function is equivalent to calling   \nderivative(f::Function, x::AbstractFloat, ::CentralMode)\n, but leaves the   selection of the finite difference mode up to the package author's   discretion.\n\n\nUse \nderivative!(output::AbstractArray, f::Function, x::AbstractFloat)\n to   approximate the derivative of \nf\n at \nx\n and store the result into the   \noutput\n array.\n\n\nUse \nderivative(f::Function)\n to generate a new function that approximates   the true derivative function of \nf\n. The new function \nf_prime\n can be   evaluated at any point \nx\n that is desired after it is constructed.\n\n\n\n\n\n\nDetailed Method-Level Documentation\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nMethod\n.\n\n\nderivative(f::Function, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using finite differences without specifying the mode of finite differences. Defaults to using central finite differences, which is equivalent to the user calling \nderivative(f, x, CentralMode())\n instead of \nderivative(f, x)\n.\n\n\nSee the documentation for \nderivative(f, x, CentralMode())\n for more details.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n\n\ny\n \n=\n \nderivative\n(\nsin\n,\n \n0.0\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative!\n \n \nFunction\n.\n\n\nderivative!(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractFloat,\n    m::Mode = CentralMode(),\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using finite differences. The first argument \noutput\n will be mutated so that its first element will contain the result.\n\n\nSee the documentation for the non-mutating versions of \nderivative\n for additional information about the effects of choosing a specific mode of finite differences.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array whose first element will be mutated.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\nm::Mode\n: An instance of the \nMode\n type. This will determine the mode   of finite differences that will be used.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nCentralMode\n\n\ny\n \n=\n \nArray\n(\nFloat64\n,\n \n1\n)\n\n\nderivative!\n(\ny\n,\n \nsin\n,\n \n0.0\n,\n \nCentralMode\n())\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nFunction\n.\n\n\nderivative\n(\nf\n:\n:Function\n,\n \nmode\n:\n:Mode\n \n=\n \nCentralMode\n();\n \nmutates\n:\n:Bool\n=\nfalse\n)\n\n\n\n\n\n\nDescription\n\n\nConstruct a new function that will evaluate the derivative of \nf\n at any value of \nx\n using finite differences. The user can specify the mode of finite differences to use by providing a second positional argument. The user can also indicate whether to return a mutating or non-mutating function using the keyword argument, \nmutates\n, which should be either \ntrue\n or \nfalse\n.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nm::Mode\n: An instance of the \nMode\n type. This will determine the mode   of finite differences that will be used.\n\n\n\n\nKeyword Arguments\n\n\n\n\nmutates::Bool = false\n: Determine whether the resulting function will mutate   its inputs or will be a pure function.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n\n\nf\u2032\n \n=\n \nderivative\n(\nsin\n)\n\n\nf\u2032\n(\n0.0\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nMethod\n.\n\n\nderivative(f::Function, x::AbstractFloat, ::ForwardMode)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using forward finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{f(x + \\epsilon) - f(x)}{\\epsilon},\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n::ForwardMode\n: An instance of the \nForwardMode\n type.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nForwardMode\n\n\ny\n \n=\n \nderivative\n(\nsin\n,\n \n0.0\n,\n \nForwardMode\n())\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nMethod\n.\n\n\nderivative(f::Function, x::AbstractFloat, ::BackwardMode)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using backward finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{f(x) - f(x - \\epsilon)}{\\epsilon},\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n::BackwardMode\n: An instance of the \nBackwardMode\n type.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nBackwardMode\n\n\ny\n \n=\n \nderivative\n(\nsin\n,\n \n0.0\n,\n \nBackwardMode\n())\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nMethod\n.\n\n\nderivative(f::Function, x::AbstractFloat, ::CentralMode)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using central finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2 \\epsilon},\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n::CentralMode\n: An instance of the \nCentralMode\n type.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nCentralMode\n\n\ny\n \n=\n \nderivative\n(\nsin\n,\n \n0.0\n,\n \nCentralMode\n())\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.derivative\n \n \nMethod\n.\n\n\nderivative(f::Function, x::AbstractFloat, ::ComplexMode)\n\n\n\n\n\nDescription\n\n\nEvaluate the derivative of \nf\n at \nx\n using complex finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{\\operatorname{Im}(f(x + \\epsilon i))}{\\epsilon}\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be as small as possible.\n\n\nNOTE\n: This mode of finite differences will work correctly only when:\n\n\n\n\nf\n supports complex inputs.\n\n\nf\n is an analytic function in the complex analysis sense of the word.\n\n\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n::ForwardMode\n: An instance of the \nForwardMode\n type.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nderivative\n,\n \nComplexMode\n\n\ny\n \n=\n \nderivative\n(\nsin\n,\n \n0.0\n,\n \nComplexMode\n())\n\n\n\n\n\n\nsource", 
            "title": "derivative"
        }, 
        {
            "location": "/core/derivative/#description", 
            "text": "The  derivative  function is used to compute the derivative of a univariate function that maps $\\mathbb{R}$ to $\\mathbb{R}$. For example,  x -  sin(x)  is a univariate function that might be passed to the  derivative  function.  The  derivative  function comes in three core variants:   A pure function that directly computes the derivative of a function  f  at a   fixed numeric value of  x  and returns a number as a result.  A mutating function that computes the derivative of a function  f  at a fixed   numeric value of  x  and stores the result into a user-provided output   array. This function returns  nothing  since its action is focused on   mutation.  A higher-order function that constructs a new function  f_prime  that will   compute the derivative of  f  at any point  x  that is provided as an   argument to  f_prime . This newly constructed function is the return value   for this variant of the  derivative  function.   In addition, the  derivative  function allows one to chose the mode of numerical differentiation that is used to compute an approximate derivative. The default is use central finite differences, which evaluates  f  at two nearby points and provides much higher accuracy than other strategies at a cost of roughly double the amount of computation. Other modes of numerical differentiation are available, but must be opted-in to explicitly.", 
            "title": "Description"
        }, 
        {
            "location": "/core/derivative/#primary-methods", 
            "text": "The primary methods that most users will want to use are the following:   Use  derivative(f::Function, x::AbstractFloat)  to approximate the derivative   of  f  at  x . This is the pure variant described above. Calling this   function is equivalent to calling    derivative(f::Function, x::AbstractFloat, ::CentralMode) , but leaves the   selection of the finite difference mode up to the package author's   discretion.  Use  derivative!(output::AbstractArray, f::Function, x::AbstractFloat)  to   approximate the derivative of  f  at  x  and store the result into the    output  array.  Use  derivative(f::Function)  to generate a new function that approximates   the true derivative function of  f . The new function  f_prime  can be   evaluated at any point  x  that is desired after it is constructed.", 
            "title": "Primary Methods"
        }, 
        {
            "location": "/core/derivative/#detailed-method-level-documentation", 
            "text": "#  FiniteDiff.derivative     Method .  derivative(f::Function, x::AbstractFloat)  Description  Evaluate the derivative of  f  at  x  using finite differences without specifying the mode of finite differences. Defaults to using central finite differences, which is equivalent to the user calling  derivative(f, x, CentralMode())  instead of  derivative(f, x) .  See the documentation for  derivative(f, x, CentralMode())  for more details.  Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .   Returns   y::Real : The derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   derivative  y   =   derivative ( sin ,   0.0 )   source  #  FiniteDiff.derivative!     Function .  derivative!(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractFloat,\n    m::Mode = CentralMode(),\n)  Description  Evaluate the derivative of  f  at  x  using finite differences. The first argument  output  will be mutated so that its first element will contain the result.  See the documentation for the non-mutating versions of  derivative  for additional information about the effects of choosing a specific mode of finite differences.  Arguments   output::AbstractArray : An array whose first element will be mutated.  f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .  m::Mode : An instance of the  Mode  type. This will determine the mode   of finite differences that will be used.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   derivative ,   CentralMode  y   =   Array ( Float64 ,   1 )  derivative! ( y ,   sin ,   0.0 ,   CentralMode ())   source  #  FiniteDiff.derivative     Function .  derivative ( f : :Function ,   mode : :Mode   =   CentralMode ();   mutates : :Bool = false )   Description  Construct a new function that will evaluate the derivative of  f  at any value of  x  using finite differences. The user can specify the mode of finite differences to use by providing a second positional argument. The user can also indicate whether to return a mutating or non-mutating function using the keyword argument,  mutates , which should be either  true  or  false .  Arguments   f::Function : The function to be differentiated.  m::Mode : An instance of the  Mode  type. This will determine the mode   of finite differences that will be used.   Keyword Arguments   mutates::Bool = false : Determine whether the resulting function will mutate   its inputs or will be a pure function.   Examples  import   FiniteDiff :   derivative  f\u2032   =   derivative ( sin )  f\u2032 ( 0.0 )   source  #  FiniteDiff.derivative     Method .  derivative(f::Function, x::AbstractFloat, ::ForwardMode)  Description  Evaluate the derivative of  f  at  x  using forward finite differences. In mathematical notation, we calculate,   \n\\frac{f(x + \\epsilon) - f(x)}{\\epsilon},   where $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.  Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .  ::ForwardMode : An instance of the  ForwardMode  type.   Returns   y::Real : The derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   derivative ,   ForwardMode  y   =   derivative ( sin ,   0.0 ,   ForwardMode ())   source  #  FiniteDiff.derivative     Method .  derivative(f::Function, x::AbstractFloat, ::BackwardMode)  Description  Evaluate the derivative of  f  at  x  using backward finite differences. In mathematical notation, we calculate,   \n\\frac{f(x) - f(x - \\epsilon)}{\\epsilon},   where $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.  Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .  ::BackwardMode : An instance of the  BackwardMode  type.   Returns   y::Real : The derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   derivative ,   BackwardMode  y   =   derivative ( sin ,   0.0 ,   BackwardMode ())   source  #  FiniteDiff.derivative     Method .  derivative(f::Function, x::AbstractFloat, ::CentralMode)  Description  Evaluate the derivative of  f  at  x  using central finite differences. In mathematical notation, we calculate,   \n\\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2 \\epsilon},   where $\\epsilon$ is chosen to be small enough to approximate the derivative, but not so small as to suffer from extreme numerical inaccuracy.  Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .  ::CentralMode : An instance of the  CentralMode  type.   Returns   y::Real : The derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   derivative ,   CentralMode  y   =   derivative ( sin ,   0.0 ,   CentralMode ())   source  #  FiniteDiff.derivative     Method .  derivative(f::Function, x::AbstractFloat, ::ComplexMode)  Description  Evaluate the derivative of  f  at  x  using complex finite differences. In mathematical notation, we calculate,   \n\\frac{\\operatorname{Im}(f(x + \\epsilon i))}{\\epsilon}   where $\\epsilon$ is chosen to be as small as possible.  NOTE : This mode of finite differences will work correctly only when:   f  supports complex inputs.  f  is an analytic function in the complex analysis sense of the word.   Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .  ::ForwardMode : An instance of the  ForwardMode  type.   Returns   y::Real : The derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   derivative ,   ComplexMode  y   =   derivative ( sin ,   0.0 ,   ComplexMode ())   source", 
            "title": "Detailed Method-Level Documentation"
        }, 
        {
            "location": "/core/second_derivative/", 
            "text": "Description\n\n\nThe \nsecond_derivative\n function is used to compute the second derivative of a univariate function that maps $\\mathbb{R}$ to $\\mathbb{R}$. For example, \nx -\n sin(x)\n is a univariate function that might be passed to the \nsecond_derivative\n function.\n\n\nThe \nsecond_derivative\n function comes in three core variants:\n\n\n\n\nA pure function that directly computes the second derivative of a function   \nf\n at a fixed numeric value of \nx\n and returns a number as a result.\n\n\nA mutating function that computes the second derivative of a function \nf\n at   a fixed numeric value of \nx\n and stores the result into a user-provided   output array. This function returns \nnothing\n since its action is focused   on mutation.\n\n\nA higher-order function that constructs a new function \nf_prime2\n that will   compute the second derivative of \nf\n at any point \nx\n that is provided as   an argument to \nf_prime2\n. This newly constructed function is the return   value for this variant of the \nsecond_derivative\n function.\n\n\n\n\n\n\nPrimary Methods\n\n\nThe primary methods that most users will want to use are the following:\n\n\n\n\nUse \nsecond_derivative(f::Function, x::AbstractFloat)\n to approximate the   second derivative of \nf\n at \nx\n. This is the pure variant described above.\n\n\nUse \nsecond_derivative!(output::AbstractArray, f::Function, x::AbstractFloat)\n   to approximate the second derivative of \nf\n at \nx\n and store the result   into the \noutput\n array.\n\n\nUse \nsecond_derivative(f::Function)\n to generate a new function that   approximates the true second derivative function of \nf\n. The new function   \nf_prime2\n can be evaluated at any point \nx\n that is desired after it is   constructed.\n\n\n\n\n\n\nDetailed Method-Level Documentation\n\n\n#\n\n\nFiniteDiff.second_derivative\n \n \nMethod\n.\n\n\nsecond_derivative(f::Function, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nEvaluate the second derivative of \nf\n at \nx\n using finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{f(x + \\epsilon) - 2 f(x) + f(x - \\epsilon)}{\\epsilon^2},\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be small enough to approximate the second derivative, but not so small as to suffer from extreme numerical inaccuracy.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\ny::Real\n: The second derivative of \nf\n evaluated at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nsecond_derivative\n\n\ny\n \n=\n \nsecond_derivative\n(\nsin\n,\n \n1.0\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.second_derivative!\n \n \nMethod\n.\n\n\nsecond_derivative!(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractFloat,\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the second derivative of \nf\n at \nx\n using finite differences. In mathematical notation, we calculate,\n\n\n\n\n\n\\frac{f(x + \\epsilon) - 2 f(x) + f(x - \\epsilon)}{\\epsilon^2},\n\n\n\n\n\nwhere $\\epsilon$ is chosen to be small enough to approximate the second derivative, but not so small as to suffer from extreme numerical inaccuracy.\n\n\nThe first argument, \noutput\n, will be mutated so that the value of the second derivative is its first element.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array whose first element will be mutated.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractFloat\n: The point at which to evaluate the derivative of \nf\n. Its   type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\nvoid::Nothing\n: This function is called for its side-effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nsecond_derivative\n\n\ny\n \n=\n \nArray\n(\nFloat64\n,\n \n1\n)\n\n\nsecond_derivative!\n(\ny\n,\n \nsin\n,\n \n1.0\n)\n\n\n\n\n\n\nsource\n\n\n#\n\n\nFiniteDiff.second_derivative\n \n \nMethod\n.\n\n\nsecond_derivative\n(\nf\n:\n:Function\n;\n \nmutates\n:\n:Bool\n=\nfalse\n)\n\n\n\n\n\n\nDescription\n\n\nConstruct a new function that will evaluate the second derivative of \nf\n at any point \nx\n. A keyword argument specifies whether the resulting function should be mutating or non-mutating.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\n\n\nKeyword Arguments\n\n\n\n\nmutates::Bool = false\n: Determine whether the resulting function will mutate   its inputs or will be a pure function.\n\n\n\n\nReturns\n\n\n\n\nvoid::Nothing\n: This function is called for its side-effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nsecond_derivative\n\n\nf\u2032\u2032\n \n=\n \nsecond_derivative\n(\nsin\n)\n\n\nf\u2032\u2032\n(\n1.0\n)\n\n\n\n\n\n\nsource", 
            "title": "second_derivative"
        }, 
        {
            "location": "/core/second_derivative/#description", 
            "text": "The  second_derivative  function is used to compute the second derivative of a univariate function that maps $\\mathbb{R}$ to $\\mathbb{R}$. For example,  x -  sin(x)  is a univariate function that might be passed to the  second_derivative  function.  The  second_derivative  function comes in three core variants:   A pure function that directly computes the second derivative of a function    f  at a fixed numeric value of  x  and returns a number as a result.  A mutating function that computes the second derivative of a function  f  at   a fixed numeric value of  x  and stores the result into a user-provided   output array. This function returns  nothing  since its action is focused   on mutation.  A higher-order function that constructs a new function  f_prime2  that will   compute the second derivative of  f  at any point  x  that is provided as   an argument to  f_prime2 . This newly constructed function is the return   value for this variant of the  second_derivative  function.", 
            "title": "Description"
        }, 
        {
            "location": "/core/second_derivative/#primary-methods", 
            "text": "The primary methods that most users will want to use are the following:   Use  second_derivative(f::Function, x::AbstractFloat)  to approximate the   second derivative of  f  at  x . This is the pure variant described above.  Use  second_derivative!(output::AbstractArray, f::Function, x::AbstractFloat)    to approximate the second derivative of  f  at  x  and store the result   into the  output  array.  Use  second_derivative(f::Function)  to generate a new function that   approximates the true second derivative function of  f . The new function    f_prime2  can be evaluated at any point  x  that is desired after it is   constructed.", 
            "title": "Primary Methods"
        }, 
        {
            "location": "/core/second_derivative/#detailed-method-level-documentation", 
            "text": "#  FiniteDiff.second_derivative     Method .  second_derivative(f::Function, x::AbstractFloat)  Description  Evaluate the second derivative of  f  at  x  using finite differences. In mathematical notation, we calculate,   \n\\frac{f(x + \\epsilon) - 2 f(x) + f(x - \\epsilon)}{\\epsilon^2},   where $\\epsilon$ is chosen to be small enough to approximate the second derivative, but not so small as to suffer from extreme numerical inaccuracy.  Arguments   f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .   Returns   y::Real : The second derivative of  f  evaluated at  x .   Examples  import   FiniteDiff :   second_derivative  y   =   second_derivative ( sin ,   1.0 )   source  #  FiniteDiff.second_derivative!     Method .  second_derivative!(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractFloat,\n)  Description  Evaluate the second derivative of  f  at  x  using finite differences. In mathematical notation, we calculate,   \n\\frac{f(x + \\epsilon) - 2 f(x) + f(x - \\epsilon)}{\\epsilon^2},   where $\\epsilon$ is chosen to be small enough to approximate the second derivative, but not so small as to suffer from extreme numerical inaccuracy.  The first argument,  output , will be mutated so that the value of the second derivative is its first element.  Arguments   output::AbstractArray : An array whose first element will be mutated.  f::Function : The function to be differentiated.  x::AbstractFloat : The point at which to evaluate the derivative of  f . Its   type must implement  eps .   Returns   void::Nothing : This function is called for its side-effects.   Examples  import   FiniteDiff :   second_derivative  y   =   Array ( Float64 ,   1 )  second_derivative! ( y ,   sin ,   1.0 )   source  #  FiniteDiff.second_derivative     Method .  second_derivative ( f : :Function ;   mutates : :Bool = false )   Description  Construct a new function that will evaluate the second derivative of  f  at any point  x . A keyword argument specifies whether the resulting function should be mutating or non-mutating.  Arguments   f::Function : The function to be differentiated.   Keyword Arguments   mutates::Bool = false : Determine whether the resulting function will mutate   its inputs or will be a pure function.   Returns   void::Nothing : This function is called for its side-effects.   Examples  import   FiniteDiff :   second_derivative  f\u2032\u2032   =   second_derivative ( sin )  f\u2032\u2032 ( 1.0 )   source", 
            "title": "Detailed Method-Level Documentation"
        }, 
        {
            "location": "/core/gradient/", 
            "text": "Description\n\n\nThe \ngradient\n function is used to compute the derivative of a multivariate function that maps $\\mathbb{R}^n$ to $\\mathbb{R}$. For example, \nx -\n sin(x[1]) + cos(x[2])\n is a multivariate function that might be passed to the \ngradient\n function.\n\n\nThe \ngradient\n function comes in three core variants:\n\n\n\n\nA pure function that directly computes the gradient of a function \nf\n at a   fixed value of \nx\n and returns a newly allocated array as a result.\n\n\nA mutating function that computes the gradient of a function \nf\n at a fixed   value of \nx\n and stores the result into a user-provided output array. This   function returns \nnothing\n since its action is focused on mutation. To   ensure thread safety, this mutating variant also requires that a buffer   be provided that is part of the internal implementation of the function,   but can be provided explicitly to minimize unnecessary memory allocations.\n\n\nA higher-order function that constructs a new function \nf_prime\n that will   compute the gradient of \nf\n at any point \nx\n that is provided as an   argument to \nf_prime\n. This newly constructed function is the return value   for this variant of the \ngradient\n function.\n\n\n\n\nIn addition, the \ngradient\n function allows one to chose the mode of numerical differentiation that is used to compute an approximate derivative. The default is use central finite differences, which evaluates \nf\n at two nearby points for each dimension of the input array \nx\n and provides much higher accuracy than other strategies at a cost of roughly double the amount of computation. Other modes of numerical differentiation are available, but must be opted-in to explicitly.\n\n\n\n\nPrimary Methods\n\n\nThe primary methods that most users will want to use are the following:\n\n\n\n\nUse \ngradient(f::Function, x::AbstractArray)\n to approximate the gradient   of \nf\n at \nx\n. This is the pure variant described above.\n\n\nUse \ngradient!(output::AbstractArray, f::Function, x::AbstractArray, buffer::AbstractArray)\n   to approximate the gradient of \nf\n at \nx\n and store the result into the   \noutput\n array.\n\n\nUse \ngradient(f::Function)\n to generate a new function that approximates   the true gradient function of \nf\n. The new function \nf_prime\n can be   evaluated at any point \nx\n that is desired after it is constructed.\n\n\n\n\n\n\nDetailed Method-Level Documentation\n\n\n#\n\n\nFiniteDiff.gradient!\n \n \nMethod\n.\n\n\ngradient!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using finite differences. Defaults to central mode finite differences. See the documentation for that mode for more details.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient of \nf\n at \nx\n.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nbuffer::AbstractArray{T}\n: A buffer that is equivalent to \nsimilar(x)\n. Used   as a temporary copy of \nx\n that can be mutated safely during the execution   of the function.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n),\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\ngradient!\n(\n\n    \noutput\n,\n\n    \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n\n    \nx\n,\n\n    \nbuffer\n,\n\n\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient\n \n \nFunction\n.\n\n\ngradient{T \n: AbstractFloat}(\n    f::Function,\n    x::AbstractArray{T},\n    mode::Mode = CentralMode(),\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using finite differences. Defaults to central mode finite differences. See the documentation for that mode for more details.\n\n\nNOTE\n: Allocates new memory to store the output as well as memory to use as a buffer.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nm::Mode\n: An instance of the \nMode\n type. This will determine the mode   of finite differences that will be used.\n\n\n\n\nReturns\n\n\n\n\noutput::AbstractArray\n: The gradient.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nForwardMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n \n=\n \ngradient!\n(\nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n \nx\n,\n \nForwardMode\n())\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient\n \n \nFunction\n.\n\n\ngradient\n(\nf\n:\n:Function\n,\n \nmode\n:\n:Mode\n \n=\n \nCentralMode\n();\n \nmutates\n:\n:Bool\n \n=\n \nfalse\n)\n\n\n\n\n\n\nDescription\n\n\nConstruct a new function that will evaluate the gradient of \nf\n at any value of \nx\n. The user can specify the mode as a positional argument and can also indicate whether to return a mutating or non-mutating function using a keyword argument.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nm::Mode\n: An instance of the \nMode\n type. This will determine the mode   of finite differences that will be used.\n\n\n\n\nKeyword Arguments\n\n\n\n\nmutates::Bool = false\n: Determine whether the resulting function will mutate   its inputs or will be a pure function.\n\n\n\n\nReturns\n\n\n\n\ng\u2032::Function\n: A function to evaluate the gradient of \nf\n at a new point   \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nForwardMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\ng\u2032\n \n=\n \ngradient\n(\nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n \nForwardMode\n(),\n \nmutates\n \n=\n \nfalse\n)\n\n\noutput\n \n=\n \ng\u2032\n(\nx\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient!\n \n \nMethod\n.\n\n\ngradient!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::ForwardMode,\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using forward-mode finite differences. Store the results into \noutput\n. Work with a user-provided \nbuffer\n to ensure that we can work without copies, but also without mutating \nx\n. In Rust jargon, we take ownership of both \noutput\n and \nbuffer\n, but do not require ownership of \nx\n. We just need read-only access to \nx\n.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient of \nf\n at \nx\n.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nbuffer::AbstractArray{T}\n: A buffer that is equivalent to \nsimilar(x)\n. Used   as a temporary copy of \nx\n that can be mutated safely during the execution   of the function.\n\n\n::ForwardMode\n: An instance of the \nForwardMode\n type.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nForwardMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n),\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\ngradient!\n(\n\n    \noutput\n,\n\n    \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n\n    \nx\n,\n\n    \nbuffer\n,\n\n    \nForwardMode\n(),\n\n\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient!\n \n \nMethod\n.\n\n\ngradient!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::BackwardMode,\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using backward-mode finite differences. Store the results into \noutput\n. Work with a user-provided \nbuffer\n to ensure that we can work without copies, but also without mutating \nx\n. In Rust jargon, we take ownership of both \noutput\n and \nbuffer\n, but do not require ownership of \nx\n. We just need read-only access to \nx\n.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient of \nf\n at \nx\n.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nbuffer::AbstractArray{T}\n: A buffer that is equivalent to \nsimilar(x)\n. Used   as a temporary copy of \nx\n that can be mutated safely during the execution   of the function.\n\n\n::BackwardMode\n: An instance of the \nBackwardMode\n type.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nBackwardMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n),\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\ngradient!\n(\n\n    \noutput\n,\n\n    \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n\n    \nx\n,\n\n    \nbuffer\n,\n\n    \nBackwardMode\n(),\n\n\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient!\n \n \nMethod\n.\n\n\ngradient!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::CentralMode,\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using central-mode finite differences. Store the results into \noutput\n. Work with a user-provided \nbuffer\n to ensure that we can work without copies, but also without mutating \nx\n. In Rust jargon, we take ownership of both \noutput\n and \nbuffer\n, but do not require ownership of \nx\n. We just need read-only access to \nx\n.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient of \nf\n at \nx\n.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nbuffer::AbstractArray{T}\n: A buffer that is equivalent to \nsimilar(x)\n. Used   as a temporary copy of \nx\n that can be mutated safely during the execution   of the function.\n\n\n::CentralMode\n: An instance of the \nCentralMode\n type.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nCentralMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n),\n \nArray\n(\nFloat64\n,\n \n2\n)\n\n\ngradient!\n(\n\n    \noutput\n,\n\n    \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n\n    \nx\n,\n\n    \nbuffer\n,\n\n    \nCentralMode\n(),\n\n\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.gradient!\n \n \nMethod\n.\n\n\ngradient!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{Complex{T}},\n    ::ComplexMode,\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the gradient of \nf\n at \nx\n using complex-mode finite differences. Store the results into \noutput\n. Work with a user-provided \nbuffer\n to ensure that we can work without copies, but also without mutating \nx\n. In Rust jargon, we take ownership of both \noutput\n and \nbuffer\n, but do not require ownership of \nx\n. We just need read-only access to \nx\n.\n\n\nNOTE\n: The buffer must have complex elements to allow the computation of complex-mode finite differences.\n\n\nNOTE\n: This mode of finite differences will work correctly only when:\n\n\n\n\nf\n supports complex inputs.\n\n\nf\n is an analytic function in the complex analysis sense of the word.\n\n\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient of \nf\n at \nx\n.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the gradient of   \nf\n.\n\n\nbuffer::AbstractArray{Complex{T}}\n: A buffer that is equivalent to   \nsimilar(x, Complex{eltype(x)})\n. Used as a temporary copy of \nx\n that can   be mutated safely during the execution of the function.\n\n\n::ComplexMode\n: An instance of the \nComplexMode\n type.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \ngradient!\n,\n \nComplexMode\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nArray\n(\nFloat64\n,\n \n2\n),\n \nArray\n(\nComplex\n{\nFloat64\n},\n \n2\n)\n\n\ngradient!\n(\n\n    \noutput\n,\n\n    \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n\n    \nx\n,\n\n    \nbuffer\n,\n\n    \nComplexMode\n(),\n\n\n)\n\n\n\n\n\n\nsource", 
            "title": "gradient"
        }, 
        {
            "location": "/core/gradient/#description", 
            "text": "The  gradient  function is used to compute the derivative of a multivariate function that maps $\\mathbb{R}^n$ to $\\mathbb{R}$. For example,  x -  sin(x[1]) + cos(x[2])  is a multivariate function that might be passed to the  gradient  function.  The  gradient  function comes in three core variants:   A pure function that directly computes the gradient of a function  f  at a   fixed value of  x  and returns a newly allocated array as a result.  A mutating function that computes the gradient of a function  f  at a fixed   value of  x  and stores the result into a user-provided output array. This   function returns  nothing  since its action is focused on mutation. To   ensure thread safety, this mutating variant also requires that a buffer   be provided that is part of the internal implementation of the function,   but can be provided explicitly to minimize unnecessary memory allocations.  A higher-order function that constructs a new function  f_prime  that will   compute the gradient of  f  at any point  x  that is provided as an   argument to  f_prime . This newly constructed function is the return value   for this variant of the  gradient  function.   In addition, the  gradient  function allows one to chose the mode of numerical differentiation that is used to compute an approximate derivative. The default is use central finite differences, which evaluates  f  at two nearby points for each dimension of the input array  x  and provides much higher accuracy than other strategies at a cost of roughly double the amount of computation. Other modes of numerical differentiation are available, but must be opted-in to explicitly.", 
            "title": "Description"
        }, 
        {
            "location": "/core/gradient/#primary-methods", 
            "text": "The primary methods that most users will want to use are the following:   Use  gradient(f::Function, x::AbstractArray)  to approximate the gradient   of  f  at  x . This is the pure variant described above.  Use  gradient!(output::AbstractArray, f::Function, x::AbstractArray, buffer::AbstractArray)    to approximate the gradient of  f  at  x  and store the result into the    output  array.  Use  gradient(f::Function)  to generate a new function that approximates   the true gradient function of  f . The new function  f_prime  can be   evaluated at any point  x  that is desired after it is constructed.", 
            "title": "Primary Methods"
        }, 
        {
            "location": "/core/gradient/#detailed-method-level-documentation", 
            "text": "#  FiniteDiff.gradient!     Method .  gradient!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n)  Description  Evaluate the gradient of  f  at  x  using finite differences. Defaults to central mode finite differences. See the documentation for that mode for more details.  Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient of  f  at  x .  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  buffer::AbstractArray{T} : A buffer that is equivalent to  similar(x) . Used   as a temporary copy of  x  that can be mutated safely during the execution   of the function.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   gradient!  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   Array ( Float64 ,   2 ),   Array ( Float64 ,   2 )  gradient! ( \n     output , \n     x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]), \n     x , \n     buffer ,  )   source   #  FiniteDiff.gradient     Function .  gradient{T  : AbstractFloat}(\n    f::Function,\n    x::AbstractArray{T},\n    mode::Mode = CentralMode(),\n)  Description  Evaluate the gradient of  f  at  x  using finite differences. Defaults to central mode finite differences. See the documentation for that mode for more details.  NOTE : Allocates new memory to store the output as well as memory to use as a buffer.  Arguments   f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  m::Mode : An instance of the  Mode  type. This will determine the mode   of finite differences that will be used.   Returns   output::AbstractArray : The gradient.   Examples  import   FiniteDiff :   gradient! ,   ForwardMode  x   =   [ 0.0 ,   0.0 ]  output   =   gradient! ( x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]),   x ,   ForwardMode ())   source   #  FiniteDiff.gradient     Function .  gradient ( f : :Function ,   mode : :Mode   =   CentralMode ();   mutates : :Bool   =   false )   Description  Construct a new function that will evaluate the gradient of  f  at any value of  x . The user can specify the mode as a positional argument and can also indicate whether to return a mutating or non-mutating function using a keyword argument.  Arguments   f::Function : The function to be differentiated.  m::Mode : An instance of the  Mode  type. This will determine the mode   of finite differences that will be used.   Keyword Arguments   mutates::Bool = false : Determine whether the resulting function will mutate   its inputs or will be a pure function.   Returns   g\u2032::Function : A function to evaluate the gradient of  f  at a new point    x .   Examples  import   FiniteDiff :   gradient! ,   ForwardMode  x   =   [ 0.0 ,   0.0 ]  g\u2032   =   gradient ( x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]),   ForwardMode (),   mutates   =   false )  output   =   g\u2032 ( x )   source   #  FiniteDiff.gradient!     Method .  gradient!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::ForwardMode,\n)  Description  Evaluate the gradient of  f  at  x  using forward-mode finite differences. Store the results into  output . Work with a user-provided  buffer  to ensure that we can work without copies, but also without mutating  x . In Rust jargon, we take ownership of both  output  and  buffer , but do not require ownership of  x . We just need read-only access to  x .  Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient of  f  at  x .  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  buffer::AbstractArray{T} : A buffer that is equivalent to  similar(x) . Used   as a temporary copy of  x  that can be mutated safely during the execution   of the function.  ::ForwardMode : An instance of the  ForwardMode  type.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   gradient! ,   ForwardMode  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   Array ( Float64 ,   2 ),   Array ( Float64 ,   2 )  gradient! ( \n     output , \n     x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]), \n     x , \n     buffer , \n     ForwardMode (),  )   source   #  FiniteDiff.gradient!     Method .  gradient!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::BackwardMode,\n)  Description  Evaluate the gradient of  f  at  x  using backward-mode finite differences. Store the results into  output . Work with a user-provided  buffer  to ensure that we can work without copies, but also without mutating  x . In Rust jargon, we take ownership of both  output  and  buffer , but do not require ownership of  x . We just need read-only access to  x .  Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient of  f  at  x .  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  buffer::AbstractArray{T} : A buffer that is equivalent to  similar(x) . Used   as a temporary copy of  x  that can be mutated safely during the execution   of the function.  ::BackwardMode : An instance of the  BackwardMode  type.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   gradient! ,   BackwardMode  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   Array ( Float64 ,   2 ),   Array ( Float64 ,   2 )  gradient! ( \n     output , \n     x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]), \n     x , \n     buffer , \n     BackwardMode (),  )   source   #  FiniteDiff.gradient!     Method .  gradient!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n    ::CentralMode,\n)  Description  Evaluate the gradient of  f  at  x  using central-mode finite differences. Store the results into  output . Work with a user-provided  buffer  to ensure that we can work without copies, but also without mutating  x . In Rust jargon, we take ownership of both  output  and  buffer , but do not require ownership of  x . We just need read-only access to  x .  Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient of  f  at  x .  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  buffer::AbstractArray{T} : A buffer that is equivalent to  similar(x) . Used   as a temporary copy of  x  that can be mutated safely during the execution   of the function.  ::CentralMode : An instance of the  CentralMode  type.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   gradient! ,   CentralMode  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   Array ( Float64 ,   2 ),   Array ( Float64 ,   2 )  gradient! ( \n     output , \n     x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]), \n     x , \n     buffer , \n     CentralMode (),  )   source   #  FiniteDiff.gradient!     Method .  gradient!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{Complex{T}},\n    ::ComplexMode,\n)  Description  Evaluate the gradient of  f  at  x  using complex-mode finite differences. Store the results into  output . Work with a user-provided  buffer  to ensure that we can work without copies, but also without mutating  x . In Rust jargon, we take ownership of both  output  and  buffer , but do not require ownership of  x . We just need read-only access to  x .  NOTE : The buffer must have complex elements to allow the computation of complex-mode finite differences.  NOTE : This mode of finite differences will work correctly only when:   f  supports complex inputs.  f  is an analytic function in the complex analysis sense of the word.   Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient of  f  at  x .  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the gradient of    f .  buffer::AbstractArray{Complex{T}} : A buffer that is equivalent to    similar(x, Complex{eltype(x)}) . Used as a temporary copy of  x  that can   be mutated safely during the execution of the function.  ::ComplexMode : An instance of the  ComplexMode  type.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   gradient! ,   ComplexMode  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   Array ( Float64 ,   2 ),   Array ( Complex { Float64 },   2 )  gradient! ( \n     output , \n     x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]), \n     x , \n     buffer , \n     ComplexMode (),  )   source", 
            "title": "Detailed Method-Level Documentation"
        }, 
        {
            "location": "/core/hessian/", 
            "text": "Description\n\n\nThe \nhessian\n function is used to compute the Hessian of a multivariate function that maps $\\mathbb{R}^n$ to $\\mathbb{R}$. For example, \nx -\n sin(x[1]) + cos(x[2])\n is a multivariate function that might be passed to the \nhessian\n function.\n\n\nThe \nhessian\n function comes in three core variants:\n\n\n\n\nA pure function that directly computes the Hessian of a function \nf\n at a   fixed value of \nx\n and returns a newly allocated array as a result.\n\n\nA mutating function that computes the Hessian of a function \nf\n at a fixed   value of \nx\n and stores the result into a user-provided output array. This   function returns \nnothing\n since its action is focused on mutation. To   ensure thread safety, this mutating variant also requires that a buffer   be provided that is part of the internal implementation of the function,   but can be provided explicitly to minimize unnecessary memory allocations.\n\n\nA higher-order function that constructs a new function \nf_prime\n that will   compute the Hessian of \nf\n at any point \nx\n that is provided as an   argument to \nf_prime\n. This newly constructed function is the return value   for this variant of the \nhessian\n function.\n\n\n\n\n\n\nPrimary Methods\n\n\nThe primary methods that most users will want to use are the following:\n\n\n\n\nUse \nhessian(f::Function, x::AbstractArray)\n to approximate the Hessian   of \nf\n at \nx\n. This is the pure variant described above.\n\n\nUse \nhessian!(output::AbstractArray, f::Function, x::AbstractArray, buffer::AbstractArray)\n   to approximate the Hessian of \nf\n at \nx\n and store the result into the   \noutput\n array.\n\n\nUse \nhessian(f::Function)\n to generate a new function that approximates   the true Hessian function of \nf\n. The new function \nf_prime\n can be   evaluated at any point \nx\n that is desired after it is constructed.\n\n\n\n\n\n\nDetailed Method-Level Documentation\n\n\n#\n\n\nFiniteDiff.hessian!\n \n \nMethod\n.\n\n\nhessian!{T \n: AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n)\n\n\n\n\n\nDescription\n\n\nEvaluate the Hessian of \nf\n at \nx\n using finite differences. Store the results into \noutput\n. Work with a user-provided \nbuffer\n to ensure that we can work without copies, but also without mutating \nx\n. In Rust jargon, we take ownership of both \noutput\n and \nbuffer\n, but do not require ownership of \nx\n. We just need read-only access to \nx\n.\n\n\nArguments\n\n\n\n\noutput::AbstractArray\n: An array that will be mutated to contain the   gradient.\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T}\n: The value of \nx\n at which to evaluate the Hessian of   \nf\n.\n\n\nbuffer::AbstractArray{T}\n: A buffer that is equivalent to \nsimilar(x)\n. Used   as a temporary copy of \nx\n that can be mutated.\n\n\n\n\nReturns\n\n\n\n\nnothing::Void\n: This function is called for its side effects.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nhessian!\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\noutput\n,\n \nbuffer\n \n=\n \nsimilar\n(\nx\n,\n \n2\n,\n \n2\n),\n \nsimilar\n(\nx\n,\n \n2\n)\n\n\nhessian!\n(\noutput\n,\n \nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n \nx\n,\n \nbuffer\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.hessian\n \n \nMethod\n.\n\n\nhessian{T \n: AbstractFloat}(f::Function, x::AbstractArray{T})\n\n\n\n\n\nDescription\n\n\nEvaluate the Hessian of \nf\n at \nx\n using finite differences. See \nhessian!(f, x, buffer)\n for more details.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\nx::AbstractArray{T \n: AbstractArray}\n: The value of \nx\n at which to evaluate   the Hessian of \nf\n.\n\n\n\n\nReturns\n\n\n\n\noutput::Array{T \n: AbstractArray}\n: The Hessian of \nf\n at \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nhessian\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\nH\n \n=\n \nhessian\n(\nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n \nx\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.hessian\n \n \nMethod\n.\n\n\nhessian\n(\nf\n:\n:Function\n;\n \nmutates\n:\n:Bool\n \n=\n \nfalse\n)\n\n\n\n\n\n\nDescription\n\n\nConstruct a new function that will evaluate the Hessian of \nf\n at any value of \nx\n. The user can whether to return a mutating or non-mutating function using a keyword argument.\n\n\nArguments\n\n\n\n\nf::Function\n: The function to be differentiated.\n\n\n\n\nKeyword Arguments\n\n\n\n\nmutates::Bool = false\n: Determine whether the resulting function will mutate   its inputs or will be a pure function.\n\n\n\n\nReturns\n\n\n\n\nh::Function\n: A function that will evaluate the Hessian of \nf\n at any \nx\n.\n\n\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nhessian\n\n\nh\n \n=\n \nhessian\n(\nx\n \n-\n \nsin\n(\nx\n[\n1\n])\n \n+\n \n2\n \n*\n \nsin\n(\nx\n[\n2\n]),\n \nmutates\n \n=\n \nfalse\n)\n\n\nx\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\nh\n(\nx\n)\n\n\n\n\n\n\nsource", 
            "title": "hessian"
        }, 
        {
            "location": "/core/hessian/#description", 
            "text": "The  hessian  function is used to compute the Hessian of a multivariate function that maps $\\mathbb{R}^n$ to $\\mathbb{R}$. For example,  x -  sin(x[1]) + cos(x[2])  is a multivariate function that might be passed to the  hessian  function.  The  hessian  function comes in three core variants:   A pure function that directly computes the Hessian of a function  f  at a   fixed value of  x  and returns a newly allocated array as a result.  A mutating function that computes the Hessian of a function  f  at a fixed   value of  x  and stores the result into a user-provided output array. This   function returns  nothing  since its action is focused on mutation. To   ensure thread safety, this mutating variant also requires that a buffer   be provided that is part of the internal implementation of the function,   but can be provided explicitly to minimize unnecessary memory allocations.  A higher-order function that constructs a new function  f_prime  that will   compute the Hessian of  f  at any point  x  that is provided as an   argument to  f_prime . This newly constructed function is the return value   for this variant of the  hessian  function.", 
            "title": "Description"
        }, 
        {
            "location": "/core/hessian/#primary-methods", 
            "text": "The primary methods that most users will want to use are the following:   Use  hessian(f::Function, x::AbstractArray)  to approximate the Hessian   of  f  at  x . This is the pure variant described above.  Use  hessian!(output::AbstractArray, f::Function, x::AbstractArray, buffer::AbstractArray)    to approximate the Hessian of  f  at  x  and store the result into the    output  array.  Use  hessian(f::Function)  to generate a new function that approximates   the true Hessian function of  f . The new function  f_prime  can be   evaluated at any point  x  that is desired after it is constructed.", 
            "title": "Primary Methods"
        }, 
        {
            "location": "/core/hessian/#detailed-method-level-documentation", 
            "text": "#  FiniteDiff.hessian!     Method .  hessian!{T  : AbstractFloat}(\n    output::AbstractArray,\n    f::Function,\n    x::AbstractArray{T},\n    buffer::AbstractArray{T},\n)  Description  Evaluate the Hessian of  f  at  x  using finite differences. Store the results into  output . Work with a user-provided  buffer  to ensure that we can work without copies, but also without mutating  x . In Rust jargon, we take ownership of both  output  and  buffer , but do not require ownership of  x . We just need read-only access to  x .  Arguments   output::AbstractArray : An array that will be mutated to contain the   gradient.  f::Function : The function to be differentiated.  x::AbstractArray{T} : The value of  x  at which to evaluate the Hessian of    f .  buffer::AbstractArray{T} : A buffer that is equivalent to  similar(x) . Used   as a temporary copy of  x  that can be mutated.   Returns   nothing::Void : This function is called for its side effects.   Examples  import   FiniteDiff :   hessian!  x   =   [ 0.0 ,   0.0 ]  output ,   buffer   =   similar ( x ,   2 ,   2 ),   similar ( x ,   2 )  hessian! ( output ,   x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]),   x ,   buffer )   source   #  FiniteDiff.hessian     Method .  hessian{T  : AbstractFloat}(f::Function, x::AbstractArray{T})  Description  Evaluate the Hessian of  f  at  x  using finite differences. See  hessian!(f, x, buffer)  for more details.  Arguments   f::Function : The function to be differentiated.  x::AbstractArray{T  : AbstractArray} : The value of  x  at which to evaluate   the Hessian of  f .   Returns   output::Array{T  : AbstractArray} : The Hessian of  f  at  x .   Examples  import   FiniteDiff :   hessian  x   =   [ 0.0 ,   0.0 ]  H   =   hessian ( x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]),   x )   source   #  FiniteDiff.hessian     Method .  hessian ( f : :Function ;   mutates : :Bool   =   false )   Description  Construct a new function that will evaluate the Hessian of  f  at any value of  x . The user can whether to return a mutating or non-mutating function using a keyword argument.  Arguments   f::Function : The function to be differentiated.   Keyword Arguments   mutates::Bool = false : Determine whether the resulting function will mutate   its inputs or will be a pure function.   Returns   h::Function : A function that will evaluate the Hessian of  f  at any  x .   Examples  import   FiniteDiff :   hessian  h   =   hessian ( x   -   sin ( x [ 1 ])   +   2   *   sin ( x [ 2 ]),   mutates   =   false )  x   =   [ 0.0 ,   0.0 ]  h ( x )   source", 
            "title": "Detailed Method-Level Documentation"
        }, 
        {
            "location": "/developer/internal_functions/", 
            "text": "Overview\n\n\nWe use dispatch to determine the mode of finite difference we use to perform numerical differentiation. The modes are specified using the following types:\n\n\n\n\nForwardMode\n\n\nBackwardMode\n\n\nCentralMode\n\n\nComplexMode\n\n\nHessianMode\n\n\n\n\nGiven a mode and a value of \nx\n at which to differentiate \nf(x)\n, the following functions are used to determine the step-size to use for finite difference approximations of derivatives:\n\n\n\n\nstep_size(::ForwardMode, x::AbstractFloat)\n\n\nstep_size(::BackwardMode, x::AbstractFloat)\n\n\nstep_size(::CentralMode, x::AbstractFloat)\n\n\nstep_size(::ComplexMode, x::AbstractFloat)\n\n\nstep_size(::HessianMode, x::AbstractFloat)\n\n\n\n\nUnless there are bugs in the definitions of the core functions, the quality of an approximate derivative is almost entirely determined by these step-sizes. These step sizes are chosen to reach a reasonable compromise between errors introduced by floating-point truncation (which could be minimized by using larger step-sizes) and errors introduced by the use of an truncated Taylor series (which could be minimized by using smaller step-sizes). The exact compromise depends on the mode of finite differences we use.\n\n\n\n\nDetailed Method-Level Documentation\n\n\n#\n\n\nFiniteDiff.step_size\n \n \nMethod\n.\n\n\nstep_size(::ForwardMode, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nDetermine the step-size to use for forward-mode finite differences.\n\n\nArguments\n\n\n\n\n::ForwardMode\n: An instance of the \nForwardMode\n type.\n\n\nx::AbstractFloat\n: A point at which the derivative of a function will be   approximated. Its type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\n\u03f5::Real\n: The step-size to use for finite differences at \nx\n.\n\n\n\n\nReferences\n\n\nSee Section 5.7 of Numerical Recipes in C for the mathematical justification for working with \nsqrt(eps(typeof(x)))\n.\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nstep_size\n,\n \nForwardMode\n\n\n\u03f5\n \n=\n \nstep_size\n(\nForwardMode\n(),\n \n0.0\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.step_size\n \n \nMethod\n.\n\n\nstep_size(::BackwardMode, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nDetermine the step-size to use for backward-mode finite differences.\n\n\nArguments\n\n\n\n\n::BackwardMode\n: An instance of the \nBackwardMode\n type.\n\n\nx::AbstractFloat\n: A point at which the derivative of a function will be   approximated. Its type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\n\u03f5::Real\n: The step-size to use for finite differences at \nx\n.\n\n\n\n\nReferences\n\n\nSee Section 5.7 of Numerical Recipes in C for the mathematical justification for working with \nsqrt(eps(typeof(x)))\n.\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nstep_size\n,\n \nBackwardMode\n\n\n\u03f5\n \n=\n \nstep_size\n(\nBackwardMode\n(),\n \n0.0\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.step_size\n \n \nMethod\n.\n\n\nstep_size(::CentralMode, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nDetermine the step-size to use for central-mode finite differences.\n\n\nArguments\n\n\n\n\n::CentralMode\n: An instance of the \nCentralMode\n type.\n\n\nx::AbstractFloat\n: A point at which the derivative of a function will be   approximated. Its type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\n\u03f5::Real\n: The step-size to use for finite differences at \nx\n.\n\n\n\n\nReferences\n\n\nSee Section 5.7 of Numerical Recipes in C for the mathematical justification for working with \ncbrt(eps(typeof(x)))\n.\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nstep_size\n,\n \nCentralMode\n\n\n\u03f5\n \n=\n \nstep_size\n(\nCentralMode\n(),\n \n0.0\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.step_size\n \n \nMethod\n.\n\n\nstep_size(::ComplexMode, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nDetermine the step-size to use for complex-mode finite differences.\n\n\nArguments\n\n\n\n\n::ComplexMode\n: An instance of the \nComplexMode\n type.\n\n\nx::AbstractFloat\n: A point at which the derivative of a function will be   approximated. Its type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\n\u03f5::Real\n: The step-size to use for finite differences at \nx\n.\n\n\n\n\nReferences\n\n\nSee \"The Complex-Step Derivative Approximation\" by Martins, Sturdza and Alonso (2003) for the mathematical justification for working with \neps(x)\n.\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nstep_size\n,\n \nComplexMode\n\n\n\u03f5\n \n=\n \nstep_size\n(\nComplexMode\n(),\n \n0.0\n)\n\n\n\n\n\n\nsource\n\n\n\n\n#\n\n\nFiniteDiff.step_size\n \n \nMethod\n.\n\n\nstep_size(::HessianMode, x::AbstractFloat)\n\n\n\n\n\nDescription\n\n\nDetermine the step-size to use for finite differences of hessians.\n\n\nArguments\n\n\n\n\n::HessianMode\n: An instance of the \nHessianMode\n type.\n\n\nx::AbstractFloat\n: A point at which the derivative of a function will be   approximated. Its type must implement \neps\n.\n\n\n\n\nReturns\n\n\n\n\n\u03f5::Real\n: The step-size to use for finite differences at \nx\n.\n\n\n\n\nReferences\n\n\nSee Section 5.7 of Numerical Recipes in C for the mathematical justification for working with \neps(x)^(1 // 4)\n.\n\n\nExamples\n\n\nimport\n \nFiniteDiff\n:\n \nstep_size\n,\n \nHessianMode\n\n\n\u03f5\n \n=\n \nstep_size\n(\nHessianMode\n(),\n \n0.0\n)\n\n\n\n\n\n\nsource", 
            "title": "Internal Types and Functions"
        }, 
        {
            "location": "/developer/internal_functions/#overview", 
            "text": "We use dispatch to determine the mode of finite difference we use to perform numerical differentiation. The modes are specified using the following types:   ForwardMode  BackwardMode  CentralMode  ComplexMode  HessianMode   Given a mode and a value of  x  at which to differentiate  f(x) , the following functions are used to determine the step-size to use for finite difference approximations of derivatives:   step_size(::ForwardMode, x::AbstractFloat)  step_size(::BackwardMode, x::AbstractFloat)  step_size(::CentralMode, x::AbstractFloat)  step_size(::ComplexMode, x::AbstractFloat)  step_size(::HessianMode, x::AbstractFloat)   Unless there are bugs in the definitions of the core functions, the quality of an approximate derivative is almost entirely determined by these step-sizes. These step sizes are chosen to reach a reasonable compromise between errors introduced by floating-point truncation (which could be minimized by using larger step-sizes) and errors introduced by the use of an truncated Taylor series (which could be minimized by using smaller step-sizes). The exact compromise depends on the mode of finite differences we use.", 
            "title": "Overview"
        }, 
        {
            "location": "/developer/internal_functions/#detailed-method-level-documentation", 
            "text": "#  FiniteDiff.step_size     Method .  step_size(::ForwardMode, x::AbstractFloat)  Description  Determine the step-size to use for forward-mode finite differences.  Arguments   ::ForwardMode : An instance of the  ForwardMode  type.  x::AbstractFloat : A point at which the derivative of a function will be   approximated. Its type must implement  eps .   Returns   \u03f5::Real : The step-size to use for finite differences at  x .   References  See Section 5.7 of Numerical Recipes in C for the mathematical justification for working with  sqrt(eps(typeof(x))) .  Examples  import   FiniteDiff :   step_size ,   ForwardMode  \u03f5   =   step_size ( ForwardMode (),   0.0 )   source   #  FiniteDiff.step_size     Method .  step_size(::BackwardMode, x::AbstractFloat)  Description  Determine the step-size to use for backward-mode finite differences.  Arguments   ::BackwardMode : An instance of the  BackwardMode  type.  x::AbstractFloat : A point at which the derivative of a function will be   approximated. Its type must implement  eps .   Returns   \u03f5::Real : The step-size to use for finite differences at  x .   References  See Section 5.7 of Numerical Recipes in C for the mathematical justification for working with  sqrt(eps(typeof(x))) .  Examples  import   FiniteDiff :   step_size ,   BackwardMode  \u03f5   =   step_size ( BackwardMode (),   0.0 )   source   #  FiniteDiff.step_size     Method .  step_size(::CentralMode, x::AbstractFloat)  Description  Determine the step-size to use for central-mode finite differences.  Arguments   ::CentralMode : An instance of the  CentralMode  type.  x::AbstractFloat : A point at which the derivative of a function will be   approximated. Its type must implement  eps .   Returns   \u03f5::Real : The step-size to use for finite differences at  x .   References  See Section 5.7 of Numerical Recipes in C for the mathematical justification for working with  cbrt(eps(typeof(x))) .  Examples  import   FiniteDiff :   step_size ,   CentralMode  \u03f5   =   step_size ( CentralMode (),   0.0 )   source   #  FiniteDiff.step_size     Method .  step_size(::ComplexMode, x::AbstractFloat)  Description  Determine the step-size to use for complex-mode finite differences.  Arguments   ::ComplexMode : An instance of the  ComplexMode  type.  x::AbstractFloat : A point at which the derivative of a function will be   approximated. Its type must implement  eps .   Returns   \u03f5::Real : The step-size to use for finite differences at  x .   References  See \"The Complex-Step Derivative Approximation\" by Martins, Sturdza and Alonso (2003) for the mathematical justification for working with  eps(x) .  Examples  import   FiniteDiff :   step_size ,   ComplexMode  \u03f5   =   step_size ( ComplexMode (),   0.0 )   source   #  FiniteDiff.step_size     Method .  step_size(::HessianMode, x::AbstractFloat)  Description  Determine the step-size to use for finite differences of hessians.  Arguments   ::HessianMode : An instance of the  HessianMode  type.  x::AbstractFloat : A point at which the derivative of a function will be   approximated. Its type must implement  eps .   Returns   \u03f5::Real : The step-size to use for finite differences at  x .   References  See Section 5.7 of Numerical Recipes in C for the mathematical justification for working with  eps(x)^(1 // 4) .  Examples  import   FiniteDiff :   step_size ,   HessianMode  \u03f5   =   step_size ( HessianMode (),   0.0 )   source", 
            "title": "Detailed Method-Level Documentation"
        }, 
        {
            "location": "/developer/code_walkthrough/", 
            "text": "Description of Individual Source Code Files\n\n\nThe code for this package exists in six files:\n\n\n\n\nmodes.jl\n\n\nThis file contains the type definitions of the finite difference modes we provide, which are \nForwardMode\n, \nBackwardMode\n, \nCentralMode\n, \nComplexMode\n and \nHessianMode\n. This code will be easier to understand if you delete the documentation before reading it because the raw code is extremely simple and the majority of the file's content is documentation.\n\n\n\n\nstep_size.jl\n\n\nThis file defines the methods of the step-size function, which determines the step-size used to compute a finite difference approximation of derivatives. The step-size is determined by both the finite difference and the value of \nx\n at which the derivative will be approximated. This code will be easier to understand if you ignore the documentation at first, because the code is very simple. But understanding why the code is the way it is will require that you read some of the literature referenced in the bibliography.\n\n\n\n\nderivative.jl\n\n\nThis file defines the methods of the \nderivative\n function, which comes in pure, mutating and higher-order variants. This file also contains definitions for each mode of finite differences that's supported. The core logic for is defined in the pure variants; the other variants are wrappers around this core logic.\n\n\n\n\nsecond_derivative.jl\n\n\nThis file defines the methods of the \nsecond_derivative\n function, which comes in pure, mutating and higher-order variants. Only one mode of finite differences is supported. The core logic is defined in the pure variants; the other variants are wrappers around this core logic.\n\n\n\n\ngradient.jl\n\n\nThis file defines the methods of the \ngradient\n function, which comes in pure, mutating and higher-order variants. This file also contains definitions for each mode of finite differences modes that's supported. Unlike the univariate functions, the core logic for \ngradient\n is defined in the mutating variants; the other variants are wrappers around this core logic.\n\n\n\n\nhessian.jl\n\n\nThis file defines the methods of the \nhessian\n function, which comes in pure, mutating and higher-order variants. Only one mode of finite differences is supported. Unlike the univariate functions, the core logic for \nhessian\n is defined in the mutating variants; the other variants are wrappers around this core logic.", 
            "title": "Code Walkthrough"
        }, 
        {
            "location": "/developer/code_walkthrough/#description-of-individual-source-code-files", 
            "text": "The code for this package exists in six files:", 
            "title": "Description of Individual Source Code Files"
        }, 
        {
            "location": "/developer/code_walkthrough/#modesjl", 
            "text": "This file contains the type definitions of the finite difference modes we provide, which are  ForwardMode ,  BackwardMode ,  CentralMode ,  ComplexMode  and  HessianMode . This code will be easier to understand if you delete the documentation before reading it because the raw code is extremely simple and the majority of the file's content is documentation.", 
            "title": "modes.jl"
        }, 
        {
            "location": "/developer/code_walkthrough/#step_sizejl", 
            "text": "This file defines the methods of the step-size function, which determines the step-size used to compute a finite difference approximation of derivatives. The step-size is determined by both the finite difference and the value of  x  at which the derivative will be approximated. This code will be easier to understand if you ignore the documentation at first, because the code is very simple. But understanding why the code is the way it is will require that you read some of the literature referenced in the bibliography.", 
            "title": "step_size.jl"
        }, 
        {
            "location": "/developer/code_walkthrough/#derivativejl", 
            "text": "This file defines the methods of the  derivative  function, which comes in pure, mutating and higher-order variants. This file also contains definitions for each mode of finite differences that's supported. The core logic for is defined in the pure variants; the other variants are wrappers around this core logic.", 
            "title": "derivative.jl"
        }, 
        {
            "location": "/developer/code_walkthrough/#second_derivativejl", 
            "text": "This file defines the methods of the  second_derivative  function, which comes in pure, mutating and higher-order variants. Only one mode of finite differences is supported. The core logic is defined in the pure variants; the other variants are wrappers around this core logic.", 
            "title": "second_derivative.jl"
        }, 
        {
            "location": "/developer/code_walkthrough/#gradientjl", 
            "text": "This file defines the methods of the  gradient  function, which comes in pure, mutating and higher-order variants. This file also contains definitions for each mode of finite differences modes that's supported. Unlike the univariate functions, the core logic for  gradient  is defined in the mutating variants; the other variants are wrappers around this core logic.", 
            "title": "gradient.jl"
        }, 
        {
            "location": "/developer/code_walkthrough/#hessianjl", 
            "text": "This file defines the methods of the  hessian  function, which comes in pure, mutating and higher-order variants. Only one mode of finite differences is supported. Unlike the univariate functions, the core logic for  hessian  is defined in the mutating variants; the other variants are wrappers around this core logic.", 
            "title": "hessian.jl"
        }, 
        {
            "location": "/examples/mle/", 
            "text": "In this example, we show how to use FiniteDiff.jl to compute the maximum likelihood estimates for a univariate logistic regression model. We'll use FiniteDiff to approximate the gradients as part of our model fitting process. We'll also use FiniteDiff to compute an approximate Hessian, which we will use as an approximation of the observed Fisher information matrix that defines the standard errors for the logistic regression model. At the end, we'll compare our results with results computed in R as a way of showing that our work is correct \u2013 and that the approximations introduced by FiniteDiff are not the dominant source of errors in these computations.\n\n\nThe first thing we'll do is import all of the libraries we'll use. If you do not have these libraries installed, you'll need to use \nPkg.add\n to install them before you can continue on with this example.\n\n\nimport\n \nDistributions\n:\n \nNormal\n,\n \nBernoulli\n\n\nimport\n \nFiniteDiff\n:\n \ngradient\n,\n \nhessian\n\n\nimport\n \nOptim\n:\n \noptimize\n,\n \nLBFGS\n,\n \nminimizer\n\n\nimport\n \nRCall\n:\n \n@\nR_str\n\n\n\n\n\n\nThe link function for a logistic regression model is the inverse logit, so we define a function to compute this quantity:\n\n\ninvlogit\n(\nz\n)\n \n=\n \n1\n \n/\n \n(\n1\n \n+\n \nexp\n(\n-\nz\n))\n\n\n\n\n\n\nNext, we'll define a function to simulate data from a logistic regression model:\n\n\nfunction\n simulate_logistic_regression\n(\nn_obs\n,\n \n\u03b2\u2080\n,\n \n\u03b2\u2081\n)\n\n    \nx\n \n=\n \nrand\n(\nNormal\n(\n0\n,\n \n1\n),\n \nn_obs\n)\n\n\n    \ny\n \n=\n \nArray\n(\nFloat64\n,\n \nn_obs\n)\n\n    \nfor\n \ni\n \nin\n \n1\n:\nn_obs\n\n        \nz_i\n \n=\n \n\u03b2\u2080\n \n+\n \n\u03b2\u2081\n \n*\n \nx\n[\ni\n]\n\n        \np_i\n \n=\n \ninvlogit\n(\nz_i\n)\n\n        \ny\n[\ni\n]\n \n=\n \nrand\n(\nBernoulli\n(\np_i\n))\n\n    \nend\n\n\n    \nreturn\n \nx\n,\n \ny\n\n\nend\n\n\n\n\n\n\nOnce we have data available, we can define the negative log likelihood function using a closure that is a function of the parameter vector \n\u0398\n, but which wraps the observed values of \nx\n and \ny\n so that \n\u0398\n is the function's only argument:\n\n\nfunction\n make_nll\n(\nx\n,\n \ny\n)\n\n    \nfunction\n nll\n(\n\u03b8\n)\n\n        \ns\n \n=\n \n0.0\n\n        \nfor\n \ni\n \nin\n \n1\n:\nlength\n(\nx\n)\n\n            \np\n \n=\n \ninvlogit\n(\n\u03b8\n[\n1\n]\n \n+\n \n\u03b8\n[\n2\n]\n \n*\n \nx\n[\ni\n])\n\n            \ns\n \n-=\n \nifelse\n(\ny\n[\ni\n]\n \n==\n \n1.0\n,\n \nlog\n(\np\n),\n \nlog\n(\n1\n \n-\n \np\n))\n\n        \nend\n\n        \ns\n\n    \nend\n\n\n    \nreturn\n \nnll\n\n\nend\n\n\n\n\n\n\nWe'll also want to be able to compute gradients. This is the first place in which we'll use FiniteDiff. Because Optim requires that the gradient function employ a different calling convention than the gradient functions that are generated by FiniteDiff, we'll need to wrap the results in a calling-convention translation layer:\n\n\nfunction\n make_nll_gr\n!\n(\nnll\n,\n \ninitial_x\n)\n\n    \ntmp!\n \n=\n \ngradient\n(\nnll\n,\n \nmutates\n=\ntrue\n)\n\n    \nbuffer\n \n=\n \nsimilar\n(\ninitial_x\n)\n\n\n    \nnll_gr!\n \n=\n \n(\nx\n,\n \ngr\n)\n \n-\n \nbegin\n\n        \ntmp!\n(\ngr\n,\n \nx\n,\n \nbuffer\n);\n\n        \nreturn\n\n    \nend\n\n\n    \nreturn\n \nnll_gr!\n\n\nend\n\n\n\n\n\n\nNow that we have the core machinery for model fitting in place, we can simulate some data and fit our model to it:\n\n\n# Simulate data\n\n\nx\n,\n \ny\n \n=\n \nsimulate_logistic_regression\n(\n2500\n,\n \n0.017\n,\n \n0.217\n)\n\n\n\n# Construct the negative log likelihood function.\n\n\nnll\n \n=\n \nmake_nll\n(\nx\n,\n \ny\n)\n\n\n\n# Initialize our model parameters as a vector of all zeros.\n\n\ninitial_x\n \n=\n \n[\n0.0\n,\n \n0.0\n]\n\n\n\n# Construct the gradient of the negative log likelihood (NLL) function.\n\n\nnll_gr!\n \n=\n \nmake_nll_gr!\n(\nnll\n,\n \ninitial_x\n)\n\n\n\n# Pass the NLL function and its gradient to Optim.optimize to find the MLE.\n\n\nres\n \n=\n \noptimize\n(\nnll\n,\n \nnll_gr!\n,\n \ninitial_x\n,\n \nLBFGS\n())\n\n\n\n# Extract the coefficients as the minimizer of the NLL function.\n\n\ncoefficients\n \n=\n \nminimizer\n(\nres\n)\n\n\n\n# Evaluate the Hessian of the NLL function at the coefficients to approximate\n\n\n#     the Fisher information matrix.\n\n\nH\n \n=\n \nhessian\n(\nnll\n,\n \ncoefficients\n)\n\n\n\n# Use an analytic result relating the Hessian to the standard errors to compute\n\n\n#     the standard errors for each coefficient.\n\n\nstandard_errors\n \n=\n \nsqrt\n(\ndiag\n(\ninv\n(\nH\n)))\n\n\n\n# Write our data to disk so we can confirm our results in R.\n\n\nwritecsv\n(\nexample_data.csv\n,\n \nhcat\n(\nx\n,\n \ny\n))\n\n\n\n# Use the RCall package\ns R macro to run R\ns glm function, which matches our\n\n\n# results.\n\n\nR\n\n\nsummary(\n\n\n    glm(\n\n\n        V2 ~ V1,\n\n\n        data = read.csv(\nexample_data.csv\n, header = FALSE),\n\n\n        family = binomial(link = \nlogit\n)\n\n\n    )\n\n\n)\n\n\n\n\n\n# Before terminating the program, clean up the file we created.\n\n\nrm\n(\nexample_data.csv\n)\n\n\n\n\n\n\nFor readers interested in model fitting, we hope this example illustrates the power of FiniteDiff for approximating gradients and Hessians. In this example, it is easy to look up analytic forms for the gradient and Hessian, but in more complicated models the use of numeric differentiation can save time while testing out new models.", 
            "title": "Maximum Likelihood Estimation"
        }, 
        {
            "location": "/bibliography/", 
            "text": "Reading Material\n\n\nA good introduction to numerical differentiation via finite differences is on \nWikipedia\n. After reading that article, we suggest reading Section 5.7 in \"Numeric Recipes in C\", which has a full citation below.\n\n\nIn addition, there is interesting material in the SciPy proposal to add support for numerical differentiation to SciPy. The proposal can be found on \nGitHub\n.\n\n\n\n\nBibliography\n\n\n\n\nPress, William H., et al. \"Numerical recipes in C\". Vol. 2. Cambridge:   Cambridge University Press, 1996.\n\n\nIott, Jocelyn, Raphael T. Haftka, and Howard M. Adelman. \"Selecting step   sizes in sensitivity analysis by finite differences.\" (1985).\n\n\nMartins, Joaquim RRA, Peter Sturdza, and Juan J. Alonso. \"The complex-step   derivative approximation.\" ACM Transactions on Mathematical Software (TOMS)   29.3 (2003): 245-262.\n\n\nLyness, James N., and Cleve B. Moler. \"Numerical differentiation of analytic   functions.\" SIAM Journal on Numerical Analysis 4.2 (1967): 202-210.   APA", 
            "title": "Bibliography"
        }, 
        {
            "location": "/bibliography/#reading-material", 
            "text": "A good introduction to numerical differentiation via finite differences is on  Wikipedia . After reading that article, we suggest reading Section 5.7 in \"Numeric Recipes in C\", which has a full citation below.  In addition, there is interesting material in the SciPy proposal to add support for numerical differentiation to SciPy. The proposal can be found on  GitHub .", 
            "title": "Reading Material"
        }, 
        {
            "location": "/bibliography/#bibliography", 
            "text": "Press, William H., et al. \"Numerical recipes in C\". Vol. 2. Cambridge:   Cambridge University Press, 1996.  Iott, Jocelyn, Raphael T. Haftka, and Howard M. Adelman. \"Selecting step   sizes in sensitivity analysis by finite differences.\" (1985).  Martins, Joaquim RRA, Peter Sturdza, and Juan J. Alonso. \"The complex-step   derivative approximation.\" ACM Transactions on Mathematical Software (TOMS)   29.3 (2003): 245-262.  Lyness, James N., and Cleve B. Moler. \"Numerical differentiation of analytic   functions.\" SIAM Journal on Numerical Analysis 4.2 (1967): 202-210.   APA", 
            "title": "Bibliography"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "FiniteDiff.jl is licensed under the MIT License:\n\n\n\n\nCopyright (c) 2015: John Myles White and other contributors.\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\nThis code is derived from Calculus.jl, which has a separate license file with authorship information available \nhere\n.", 
            "title": "License"
        }
    ]
}